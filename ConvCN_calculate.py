# 计算ConvCN模型在测试集的所有评分
from model import KGEModel
from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter
from builddata import *
import tqdm
import pickle
from Paper import Paper
import json

parser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter, conflict_handler='resolve')

parser.add_argument("--data", default="./data/", help="Data sources.")
parser.add_argument("--name", default="aan", help="Name of the dataset.")
parser.add_argument("--start", default=0, type=int, help="从第n篇测试论文开始评估，用于断点续传")
parser.add_argument("--model_name", default='expNoName', help="")
parser.add_argument("--embedding_dim", default=50, type=int, help="Dimensionality of character embedding")
parser.add_argument("--ConvL1FiltersNum", default=250, type=int, help="number of filter in convolution layer 1")
parser.add_argument("--ConvL2FiltersNum", default=10, type=int, help="number of filter in convolution layer 2")
parser.add_argument("--gamma", default=2.4, type=float, help="Gamma value")
parser.add_argument("--learning_rate", default=0.01, type=float, help="Learning rate")
parser.add_argument("--batch_size", default=64, type=int, help="Batch Size")
parser.add_argument("--neg_ratio", default=1, type=float, help="Number of negative triples generated by positive")
parser.add_argument("--num_epochs", default=3000, type=int, help="Number of training epochs")
parser.add_argument("--num_splits", default=8, type=int,
                    help="Split the validation set into 8 parts for a faster evaluation")
parser.add_argument("--testIdx", default=1, type=int, help="From 0 to 7. Index of one of 8 parts")
parser.add_argument('--fold', default='1')
parser.add_argument("--dpo1", action='store_true', help="use dropout in CNN1")
parser.add_argument("--dpo2", action='store_true', help="use dropout in CNN2")
parser.add_argument("--bn1", action='store_true', help="use batch normalization in CNN1")
parser.add_argument("--bn2", action='store_true', help="use batch normalization in CNN1")
parser.add_argument("--alpha", default=0.8, type=float, help="PaperRank分数的占比，在[0.0, 1.0]之间")
parser.add_argument("--papers_num", default=12390, type=int, help="论文数量，指定实际待排序节点的id范围")
parser.add_argument("--alpha-pr", default=0.65, type=float, help="alpha in paper rank")
parser.add_argument("--useConstantInit", action='store_true')
parser.add_argument("--decode", action='store_false')
parser.add_argument("--per", default=0, type=int, help="每per个测试项就输出已得到的分数")

args = parser.parse_args()

fold = args.fold
train, test, words_indexes, indexes_words, headTailSelector, entity2id, id2entity, relation2id, id2relation \
    = custom_build_data(name=args.name, path='./data', fold=fold)

# 加载模型
kge_model = KGEModel(
    # model_name=args.model,
    # hidden_dim=args.hidden_dim,
    # hidden_dim=50,  # just for debugging (remove this line later)
    vocab_size=len(words_indexes),
    embedding_size=args.embedding_dim,
    gamma=args.gamma,
    # gamma=2.4, # just for debugging (remove this line later)
    batch_size=args.batch_size,
    neg_ratio=args.neg_ratio,
    dpo1=args.dpo1,
    dpo2=args.dpo2,
    bn1=args.bn1,
    bn2=args.bn2,
    channel1_num=args.ConvL1FiltersNum,
    channel2_num=args.ConvL2FiltersNum
)

model_path = os.path.join(os.getcwd(), 'models',
                          args.name + '_' + args.model_name + fold + '-' + str(args.num_epochs) + 'epochs' + '.pth')
# model_path = os.path.join(os.getcwd(),'models',args.name+'_'+args.model_name+fold+'.pth')
kge_model.load_state_dict(torch.load(model_path))
kge_model = kge_model.cuda()

# set all parameters not to require grad
for param in kge_model.parameters():
    param.requires_grad = False

kge_model.eval()
print('load model finished')

# 2. 加载测试机与训练集
test_papers = {}  # 每一项为 pid: Paper对象

with open(args.data + '/' + args.name + '/' + 'test_fold' + args.fold + '.txt') as f:
    lines = f.readlines()

for line in lines:
    xt = line.replace('\n', '').split(sep='\t')
    citing_id = int(xt[0])
    cited_id = int(xt[2])
    if citing_id not in test_papers:
        test_papers[citing_id] = Paper(citing_id)
    test_papers[citing_id].add_test_cited_paper(int(cited_id))

with open(args.data + '/' + args.name + '/' + 'train_fold' + args.fold + '.txt') as f:
    lines = f.readlines()

for line in lines:
    xt = line.replace('\n', '').split(sep='\t')
    citing_id = int(xt[0])
    cited_id = int(xt[2])
    if citing_id in test_papers:
        test_papers[citing_id].add_train_cited_paper(int(cited_id))


# 3. 开始计算convcn分数
def predict_score(x_batch):
    h = torch.tensor(x_batch[:, 0], dtype=torch.long).cuda().reshape(-1, 1)
    r = torch.tensor(x_batch[:, 1], dtype=torch.long).cuda().reshape(-1, 1)
    t = torch.tensor(x_batch[:, 2], dtype=torch.long).cuda().reshape(-1, 1)

    return kge_model(h, r, t, True).cpu().detach().numpy()


print('start calculate convcn score')

all_convcn_score = {}
last = 0
test_keys = sorted(test_papers.keys())  # 排序后的test_keys，确保每次遍历顺序相同
# 计算每篇测试论文对候选(训练)论文的convcn_score，存储于all_convcn_score
for cur, p in tqdm.tqdm(enumerate(test_keys[args.start:]), total=len(test_papers[args.start:])):
    citing_id = test_papers[p].id
    cited_ids = test_papers[p].test_cited_paper
    # triplets = np.ones((len(id2entity),3))
    triplets = np.tile(citing_id, (len(id2entity), 3))
    triplets[:, 1] = 1
    triplets[:, 2] = list(id2entity.keys())
    del_idx = test_papers[p].train_cited_paper
    # del_idx.append(citing_id)
    triplets = np.delete(triplets, del_idx, axis=0)
    triplets = triplets.astype(int)
    score = predict_score(triplets[:args.papers_num])  # 计算ConvCN分数
    score = 1 - ((score - np.min(score)) / np.ptp(score))  # 1 - normalized score

    score_dict = {}  # score of CNN
    combined_score = {}  # ensemble score... ConvCN-PR分数
    for i in range(min(len(triplets), args.papers_num)):
        score_dict[triplets[i, 2]] = score[i][0]  # 存储每篇论文的ConvCv得分，结构为{pid: score,...}
    all_convcn_score[p] = score_dict
    if cur % args.per == args.per-1:
        # 每per轮存一次all_convcn_score的结果
        print(f"output sore file {cur}...\n")
        with open('convcn_score/' + args.name + "_fold" + args.fold + "_" + str(last) + "to" + str(cur+1) + '.pkl', 'wb') as f:
            pickle.dump(all_convcn_score, f)
        print(f"finish output {cur}\n")
        last = cur+1
        all_convcn_score.clear()  # 清空内存

# 4. 存储convcn分数
with open('convcn_score/' + args.name + "_fold" + args.fold + "_" + str(last) + "to" + cur + '.pkl', 'wb') as f:
    pickle.dump(all_convcn_score, f)
